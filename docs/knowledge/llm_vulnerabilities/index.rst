LLM Vulnerabilities
===================

Large Language Model (LLM) vulnerabilities are different from those in traditional ML models. It's crucial to have a comprehensive understanding of these unique critical vulnerabilities that can impact your model. Giskard provides an `automatic scan functionality <../../guides/scan/index.rst>`_ that is designed to automatically detect a variety of risks associated with your LLMs. You can learn more about the different vulnerabilities it can detect here:

.. toctree::
   :maxdepth: 1

   hallucination/index
   harmfulness/index
   injection/index
   robustness/index
   formatting/index
   disclosure/index
   stereotypes/index

By conducting a `Giskard scan <../../guides/scan/index.rst>`_, you can proactively identify and address these vulnerabilities to ensure the reliability, fairness, and robustness of your LLMs.
