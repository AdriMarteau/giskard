# üß™ Customize your tests

[//]: # (TODO: shorten every section and add table of contents)

## üë®‚Äçüî¨ Test your ML model

Because ML models depend on data, testing scenarios depend on domain specificities and are often infinite. Giskard provides all the necessary tools that enable you to:
* Start writing tests by detecting the vulnerabilities of your model (see the [Scan](../../scan/index.rst) feature to automatically create tests)
* Create reproducible test suites with fixtures that integrate the domain knowledge of your model
* Load the best tests from the Giskard open-source catalog

### Execute a Giskard test

To create and execute a Giskard test, you have two possibilities. You can either load a test from the Giskard [catalog](../../../catalogs/test-catalog/index.rst) or create your own test by decorating a Python function.

:::{hint}
In order to execute the test provided by Giskard, you first need to wrap your dataset and model to make them compatible with Giskard. See the [wrap your model](../../../guides/wrap_model/index.md) and [wrap your dataset](../../../guides/wrap_dataset/index.md) sections.
:::

#### Load and execute a test from the Giskard Catalog

:::{hint}
Don't hesitate to execute the code snippets below in your notebook. You can see all our tests in the [üìñ Test Catalog](../../../catalogs/test-catalog/index.rst)
:::


##### Drift tests

Testing for drift enables you to make sure your model is still valid by checking if the predictions, and features in inference time are close to the ones used for training the model. Drift measures the difference in statistical distribution between a reference data, often the training set, and a ‚Äúcurrent‚Äù dataset such as the test set or a batch of examples in inference time. To pick the right drift tests, have a look at our resource ([numerical data drift](https://www.giskard.ai/knowledge/how-to-test-ml-models-3-n-numerical-data-drift) or [categorical data drift](https://www.giskard.ai/knowledge/how-to-test-ml-models-2-n-categorical-data-drift)) and [drift catalog](../../../reference/tests/drift.rst) pages.

Thanks to Giskard, your drift tests can **focus on specific data slices** by passing a [slicing function](../slice/index.md) (generated by the scan for example) as a parameter of your test.

```python
from giskard import demo, Model, Dataset, testing, slicing_function
import pandas as pd

model, df = demo.titanic()

wrapped_model = Model(model=model, model_type="classification")
train_df = Dataset(df=df.head(400), target="Survived", cat_columns=['Pclass', 'Sex', "SibSp", "Parch", "Embarked"])
test_df = Dataset(df=df.tail(400), target="Survived", cat_columns=['Pclass', 'Sex', "SibSp", "Parch", "Embarked"])

# Create a slicing function on females to create more domain-specific tests
@slicing_function(name="females")
def female_slice(row: pd.Series):
    return row["Sex"] == "female"


result = testing.test_drift_prediction_ks(
    model=wrapped_model,
    actual_dataset=test_df,
    reference_dataset=train_df,
    classification_label="yes",
    slicing_function=female_slice,
    threshold=0.5,
).execute()

print("Result for 'Classification Probability drift (Kolmogorov-Smirnov):")
print(f"Passed: {result.passed}")
print(f"Metric: {result.metric}")
```

##### Performance tests

Performance tests are probably the most well-known by data scientists. Using Giskard you can focus your performance tests on specific data slices by passing a [slicing function](../slice/index.md) (generated by the scan for example) as a parameter of your test. To know more about Giskard performance tests, check out our [performance catalog]( ../../../reference/tests/performance.rst).

```python
from giskard import demo, Model, Dataset, testing, slicing_function
import pandas as pd

model, df = demo.titanic()

wrapped_model = Model(model=model, model_type="classification")
wrapped_dataset = Dataset(
    df=df,
    target="Survived",
    cat_columns=["Pclass", "Sex", "SibSp", "Parch", "Embarked"],
)

# Create a slicing function on females to create more domain-specific tests
@slicing_function(name="females")
def female_slice(row: pd.Series):
    return row["Sex"] == "female"

result = testing.test_f1(
    dataset=wrapped_dataset,
    model=wrapped_model,
    slicing_function=female_slice,
).execute()

print(f"result: {result.passed} with metric {result.metric}")
```

##### Metamorphic tests

Metamorphic tests are important for checking the robustness of your model. In a nutshell, it's about assessing the behavior of your AI model after transformations (changes of input data). As an added benefit, it enables you to implement tests without knowing the ground truth label. To learn more about metamorphic testing (the theory, the different categories and examples), check out our [article](https://www.giskard.ai/knowledge/how-to-test-ml-models-4-metamorphic-testing) and [metamorphic tests catalog](../../../reference/tests/metamorphic.rst).

With Giskard, creating metamorphic tests becomes simple. You can automatically generate [transformations](../transformation-functions/index.md) with the scan feature, load them from the catalog, or create your own functions. You can then pass these transformation functions as parameters of your metamorphic test:

```python
from giskard import demo, Model, Dataset, testing, transformation_function
import pandas as pd

model, df = demo.titanic()

wrapped_model = Model(model=model, model_type="classification")
wrapped_dataset = Dataset(
    df=df,
    target="Survived",
    cat_columns=["Pclass", "Sex", "SibSp", "Parch", "Embarked"],
)

# Increase the age by 10% to check if we have more "survived" probability
@transformation_function(name="increase age")
def increase_age(row):
    row["Age"] = row["Age"] * 0.1
    return row

result = testing.test_metamorphic_invariance(
    model=wrapped_model, 
    dataset=wrapped_dataset, 
    transformation_function=increase_age
).execute()

print(f"result: {result.passed} with metric {result.metric}")
```

##### Statistic tests

Statistic tests enable you to write some heuristics on the behavior of the model. For instance, checking that the predicted probability is inside a specific range for a given data slice, or checking if an example has a specific classification label. Writing these tests are important to make sure the model has not learned noise or spurious relations. Have a look at our [catalog](../../../reference/tests/statistic.rst) of statistic tests.

```python
from giskard import demo, Model, Dataset, testing

model, df = demo.titanic()

wrapped_model = Model(model=model, model_type="classification")
wrapped_dataset = Dataset(
    df=df,
    target="Survived",
    cat_columns=["Pclass", "Sex", "SibSp", "Parch", "Embarked"],
)

# Let's check if the ratio of examples labeled as "yes" is over 0.7
result = testing.test_right_label(
    model=wrapped_model,
    dataset=wrapped_dataset,
    classification_label="yes",
    threshold=0.7,
).execute()

print(f"result: {result.passed} with metric {result.metric}")
```

#### Create and execute your own test

If the test you want to create is not in the Giskard catalog, you can easily write it and add it to a test suite. To do so, you just need to **decorate** a Python function to turn it into a Giskard test. Make sure that the Python function you're decorating:
* Has typed inputs: types can be Giskard `Model`, `Dataset`, `SlicingFunction` & `TransformationFunction` or any primitive
* Returns a `TestResult` object containing all the resulting information of the test: 
    * This object must have the `passed` argument: a boolean that is `true` if the test passes, `false` otherwise
    * Provide the `metric` argument: a `float` that reflects the test output. This is key to compare tests results

```python
from giskard import demo, test, Dataset, TestResult, testing

#Creating a data quality test checking if the frequency of a category is under a threshold
@test(name="My Example", tags=["quality", "custom"])
def uniqueness_test_function(dataset: Dataset,
                             category: str,
                             column_name: str,
                             threshold: float = 0.5): #you can put default value to the test
    freq_of_cat = dataset.df[column_name].value_counts()[category] / (len(dataset.df))
    passed = freq_of_cat < threshold

    return TestResult(passed=passed, metric=freq_of_cat)
    
#Now let's run this test to check if the frequency of "female" is under 70% 
_, df = demo.titanic()

wrapped_dataset = Dataset(
    df=df,
    target="Survived",
    cat_columns=["Pclass", "Sex", "SibSp", "Parch", "Embarked"],
)
uniqueness_test_function(dataset=wrapped_dataset, 
                         column_name = "Sex", 
                         category="female", 
                         threshold =0.7
                        ).execute()
```

### Create & Execute a suite

Test suites are a key feature of Giskard. Executing test suites can be useful for:
* **Comparing different models**: In this case, you should define your **model as input** of your test suite. Comparing different models is important:
    *  During production: If you want to **automate the retraining process** to know if the model you just created is better than the one in production
    *  During development: If you want to compare different model candidates. For example, test suites can be used to find the right hyperparameters of your model during your **cross-validation**
* **Comparing different datasets**. In this case, you should define your **dataset as input** of your test suite. Comparing different datasets is important to:
    * Detect **drift** between two datasets (i.e. training, testing, production, golden datasets)
    * **monitor** your model in production using different batches of datasets

:::{hint}
* When adding the tests to your suite, you can choose to **not specify** some of the parameters of your test function. In this case, you will need to specify these missing parameters **when you execute** the test suite.
* You can also choose to share some input of your test. This is useful if some tests are sharing the same inputs (ex: the same slicing function). In this case you'll need 
:::

::::{tab-set}

:::{tab-item} Model as suite input

Having the model as suite input enables you to compare models while in development (ex: to fine tune your model) or during production (ex: to automate retraining process). In the example below, we create a suite with two simple performance tests. As you see below, we specify all the test parameters **except the model to "expose" it as the suite input**:

```python
from giskard import demo, Model, Dataset, testing, Suite

model, df = demo.titanic()

wrapped_dataset = Dataset(
    df=df,
    target="Survived",
    cat_columns=["Pclass", "Sex", "SibSp", "Parch", "Embarked"],
)


@slicing_function()
def slice_sex(row: pd.Series, category: str):
    return row["Sex"] == category


# Create a suite and add an F1 and an accuracy test for the same slice
# Note that all the parameters are specified except model
# Which means that we will need to specify model everytime we run the suite
suite = (
    Suite()
    .add_test(
        testing.test_f1(
            dataset=wrapped_dataset, slicing_function=slice_sex(category="male")
        )
    )
    .add_test(
        testing.test_accuracy(
            dataset=wrapped_dataset, slicing_function=slice_sex(category="female")
        )
    )
)

# Create our first model
my_first_model = Model(model=model, model_type="classification")

# Run the suite by specifying our model and display the results
passed_first, results_first = suite.run(model=my_first_model)

# Create an improved version of our model
my_improved_model = Model(model=model, model_type="classification")

# Run the suite with our new version and check if the results improved
passed_second, results_second = suite.run(model=my_improved_model)
```

:::

:::{tab-item} Dataset as suite input

Having the dataset as suite input enables you to follow the behavior of different batches of datasets (monitoring) and detect drift. In the example below, we create a suite with two simple performance tests. As you can see, we specify all the test parameters except the dataset to **"expose" it as a the suite input**:

```python
import pandas as pd
from giskard import (
    demo,
    Model,
    Dataset,
    testing,
    Suite,
    transformation_function,
    slicing_function,
)

model, df = demo.titanic()

wrapped_model = Model(model=model, model_type="classification")


@slicing_function(name="females")
def slice_sex(row: pd.Series):
    return row["Sex"] == "female"


# Create our golden dataset. Golden dataset are used as the reference dataset. It can be your training set
golden = Dataset(
    df=df,
    target="Survived",
    cat_columns=["Pclass", "Sex", "SibSp", "Parch", "Embarked"],
)

# The first test focuses specifically on females to make sure there is no drift with respect to the golden dataset
# Note that neither tests specify the actual_dataset parameter
suite = (
    Suite()
    .add_test(
        testing.test_drift_prediction_ks(
            model=wrapped_model,
            slicing_function=slice_female,
            reference_dataset=golden,
            classification_label="yes",
        )
    )
    .add_test(
        testing.test_drift_prediction_ks(
            model=wrapped_model, reference_dataset=golden, classification_label="yes"
        )
    )
)


# batch_1 can be a first batch of production data
batch_1 = Dataset(
    df=df.head(100),
    target="Survived",
    cat_columns=["Pclass", "Sex", "SibSp", "Parch", "Embarked"],
)

# Run the suite by specifying our model and display the results
passed_1, results_1 = suite.run(actual_dataset=batch_1)

# batch_2 can be a second batch of production data
batch_2 = Dataset(
    df=df.tail(100),
    target="Survived",
    cat_columns=["Pclass", "Sex", "SibSp", "Parch", "Embarked"],
)

# Run the suite with our new version and check if the results improved
passed_2, results_2 = suite.run(actual_dataset=batch_2)
```
:::

:::{tab-item} Shared test input

For advanced cases, you may need to define some test inputs that are shared between different tests inside your suite. In this case, you should use the `SuiteInput` whose parameter are:
* The name of the test input (a string)
* The type of the test input (`Model`, `Dataset`, `SlicingFunction`, `TransformationFunction` or any other primitive. 

In the example below, the data slice `female` is shared between two performance tests:

```python
from giskard import (
    demo,
    Model,
    Dataset,
    testing,
    Suite,
    SuiteInput,
    slicing_function,
    SlicingFunction,
)
import pandas as pd

model, df = demo.titanic()

wrapped_model = Model(model=model, model_type="classification")
wrapped_dataset = Dataset(
    df=df,
    target="Survived",
    cat_columns=["Pclass", "Sex", "SibSp", "Parch", "Embarked"],
)


@slicing_function()
def slice_female(row: pd.Series):
    return row["Sex"] == "female"


shared_input_female = SuiteInput("female_slice", SlicingFunction)

suite = (
    Suite()
    .add_test(testing.test_auc(slicing_function=shared_input_female, threshold=0.7))
    .add_test(testing.test_f1(slicing_function=shared_input_female, threshold=0.8))
)


suite.run(model=wrapped_model, dataset=wrapped_dataset, female_slice=slice_female)
```
:::
::::

## üî™ Identify data slices

Having global quality metrics is often not enough. By analyzing subsets based on relevant factors you can identify biases, disparities, and issues specific to certain groups.

> **Warning:** Slicing functions are not sub-datasets! They are functions that can be applied to new datasets such as your production data for testing, debugging, and monitoring purposes. Saving data slices is key to integrating your **domain knowledge** to the tests.

Giskard enables you to **automatically** create slicing functions, such as *low-performing*, *underconfident*, *overconfident* or *spurious data slices*. We also propose various slicing functions in the **Giskard catalog**, such as sentiment, irony or toxicity detectors. Have a look at our [üî™ Slicing Function Catalog](../../../catalogs/slicing-function-catalog/index.rst).

This section explains how to create your own slicing function, or customize the functions generated by the Giskard scan and how save them.

### Load slicing functions from the Giskard catalog

The [Giskard catalog](../../../catalogs/slicing-function-catalog/index.rst) provides you with different slicing functions for NLP such as sentiment, hate, and toxicity detectors:

```
#Load sentiment analysis model from the Giskard catalog
from giskard.ml_worker.testing.functions.slicing import positive_sentiment_analysis
```

### Create your own slicing function

To create a Giskard slicing function, you just need to decorate an existing Python function with `@slicing_function()`. Depending on the argument of the decorator, you can decorate different Python functions filtered by row, dataframe, or cell level:

:::::{tab-set}

::::{tab-item} row_level=True (default)

When `row_level=True`, you can decorate a function that takes a pandas dataframe **row** as input and returns a boolean. Make sure that the first argument of your function corresponds to the row you want to filter:

```
from giskard import slicing_function, demo
import pandas as pd

_, df = demo.titanic()
dataset = Dataset(df=df, target="Survived", cat_columns=['Pclass', 'Sex', "SibSp", "Parch", "Embarked"])

@slicing_function(row_level=True)
def my_func2(row: pd.Series, threshold: int):
    return row['Age'] > threshold

dataset.slice(my_func2(threshold=20))
```

::::

::::{tab-item} row_level=False

When `row_level=False`, you can decorate a function that takes a full **pandas dataframe** as input and returns a filtered pandas dataframe. Make sure that the first argument of your function corresponds to the pandas dataframe you want to filter:

```
from giskard import slicing_function, demo
import pandas as pd

_, df = demo.titanic()
dataset = Dataset(df=df, target="Survived", cat_columns=['Pclass', 'Sex', "SibSp", "Parch", "Embarked"])

@slicing_function(row_level=False)
def my_func1(df: pd.DataFrame, threshold: int):
    df['Age'] = df['Age'] > threshold
    return df

dataset.slice(my_func1(threshold=20))
```

::::

::::{tab-item} cell_level=True

When `cell_level=True` (False by default), you can decorate a function that takes a **value** (string, numeric or text) as an argument and returns a boolean. Make sure that the first argument of your function corresponds to the value and that the second argument defines the **column name** where you want to filter the value:

```
from giskard import slicing_function, demo
import pandas as pd

_, df = demo.titanic()
dataset = Dataset(df=df, target="Survived", cat_columns=['Pclass', 'Sex', "SibSp", "Parch", "Embarked"])

@slicing_function(cell_level=True)
def my_func3(cell: int, threshold: int):
    return cell>threshold

train_df.slice(my_func3(threshold=20), column_name='Age')
```

::::
:::::

### AI-based slicing functions

Slicing functions can be very powerful to detect complex behaviour when they are used as fixtures inside your test suite. With the Giskard framework you can easily create complex slicing functions. For instance:

```
def _sentiment_analysis(x, column_name, threshold, model, emotion):
    from transformers import pipeline
    sentiment_pipeline = pipeline("sentiment-analysis", model=model)
    # Limit text to 512 characters
    sentences = list(map(lambda txt: txt[:512], list(x[column_name])))
    return x.iloc[list(
        map(lambda s: s['label'] == emotion and s['score'] >= threshold, sentiment_pipeline(sentences)))]
        
@slicing_function(name="Emotion sentiment", row_level=False, tags=["sentiment", "text"])
def emotion_sentiment_analysis(x: pd.DataFrame, column_name: str, emotion: str, threshold: float = 0.9) -> pd.DataFrame:
    """
    Filter the rows where the specified 'column_name' has an emotion matching 'emotion', as determined by a pre-trained sentiment analysis model.
    Possible emotion are: 'optimism', 'anger', 'sadness', 'joy'
    """
    return _sentiment_analysis(x, column_name, threshold, "cardiffnlp/twitter-roberta-base-emotion", emotion)
```
### Automatically generate some slicing functions through the scan

Giskard enables you to automatically generate the slicing functions that are the most insightul for your ML models. You can easily extract the results of the scan feature using the following code:

```
from giskard import Dataset, Model

my_dataset = Dataset(...)
my_model = Model(...)

scan_result = giskard.scan(my_model, my_dataset)
test_suite = scan_result.generate_test_suite("My first test suite")
test_suite.run()[1]
```

## üîÑ Define data transformations

Transformations such as adding typos, switching words, or paraphrasing can help create more diverse and realistic training datasets. This can enhance the model's resilience to noise and improve its performance on unseen examples.

Giskard enables you to **automatically** generate transformation functions to make your model more robust (see the [scan feature](../../scan/index.rst)). We propose various transformation functions in the **Giskard catalog**, such as *adding typos* or *punctuation
stripping*. Have a look at our [Transformation Function Catalog here](../../../catalogs/transformation-function-catalog/index.rst)

This section explains how to create your own transformation function, or customize the functions generated by the Giskard scan and how to save them.

### Load transformation functions from the Giskard catalog

The [Giskard catalog](../../../catalogs/transformation-function-catalog/index.rst) provides you with different transformation functions for NLP use cases such as *adding typos*, or *punctuation stripping*.

```
#Import keyboard typo transformations
from giskard.ml_worker.testing.functions.transformation import keyboard_typo_transformation
```

### Create your own transformation function

To create a Giskard transformation function, you just need to decorate an existing Python function with `@transformation_function()`. Depending on the argument of the decorator, you can decorate different Python functions filtered by row, dataframe or cell level:

:::::{tab-set}

::::{tab-item} row_level=True (default)

When `row_level=True`, you can decorate a function that takes a pandas dataframe **row** as input, and returns a boolean. Make sure that the first argument of your function corresponds to the row you want to filter:

```
from giskard import transformation_function, demo
import pandas as pd

_, my_df = demo.titanic()
dataset = Dataset(df=my_df, target="Survived", cat_columns=['Pclass', 'Sex', "SibSp", "Parch", "Embarked"])

@transformation_function(row_level=True)
def my_func2(row: pd.Series, offset: int):
    row['Age'] = row['Age'] + offset
    return row

transformed_dataset = dataset.transform(my_func2(offset=20))
```

::::

::::{tab-item} row_level=False

When `row_level=False`, you can decorate a function that takes a full **pandas dataframe** as input, and returns a filtered pandas dataframe. Make sure that the first argument of your function corresponds to the pandas dataframe you want to filter:

```
from giskard import transformation_function, demo
import pandas as pd

_, df = demo.titanic()
dataset = Dataset(df=df, target="Survived", cat_columns=['Pclass', 'Sex', "SibSp", "Parch", "Embarked"])

@transformation_function(row_level=False)
def my_func1(df: pd.DataFrame, offset: int):
    df['Age'] = df['Age'] + offset
    return df

transformed_dataset = dataset.transform(my_func1(offset=20))
```

::::

::::{tab-item} cell_level=True

When `cell_level=True` (False by default), you can decorate a function that takes as argument a **value** (string, numeric or text), and returns a boolean. Make sure that the first argument of your function corresponds to the value, and that the second argument defines the **column name** where you want to filter the value:

```
from giskard import transformation_function, demo
import pandas as pd

_, df = demo.titanic()
dataset = Dataset(df=df, target="Survived", cat_columns=['Pclass', 'Sex', "SibSp", "Parch", "Embarked"])

@transformation_function(cell_level=True)
def my_func3(cell: int, offset: int):
    return cell + offset

transformed_dataset = dataset.transform(my_func3(offset=20), column_name='Age')
```

::::
:::::

### AI-based tranformation function

Transformation functions can be very powerful to detect complex behaviour when they are used as fixtures inside your test suite. With the Giskard framework you can easily create complex transformation functions. For example:

```
@transformation_function(name="Change writing style", row_level=False, tags=['text'])
def change_writing_style(x: pd.DataFrame, index: int, column_name: str, style: str,
                         OPENAI_API_KEY: str) -> pd.DataFrame:
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
    rewrite_prompt_template = """
    As a text rewriting robot, your task is to rewrite a given text using a specified rewriting style. You will receive a prompt with the following format:
    ```
    "TEXT"
    ===
    "REWRITING STYLE"
    ```
    Your goal is to rewrite the provided text according to the specified style. The purpose of this task is to evaluate how the rewritten text will affect our machine learning models.

    Your response should be in the following format:
    ```
    REWRITTEN TEXT
    ```
    Please ensure that your rewritten text is grammatically correct and retains the meaning of the original text as much as possible. Good luck!
    ```
    "TEXT": {text}
    ===
    "REWRITING STYLE": {style}
    ```
    """

    from langchain import PromptTemplate
    from langchain import LLMChain
    from langchain import OpenAI

    rewrite_prompt = PromptTemplate(input_variables=['text', 'style'], template=rewrite_prompt_template)
    chain_rewrite = LLMChain(llm=OpenAI(), prompt=rewrite_prompt)

    x.at[index, column_name] = chain_rewrite.run({'text': x.at[index, column_name], 'style': style})
    return x
```
### Automatically generate some transformation functions through the scan

Giskard enables you to automatically generate the transformation functions that are the most insightul for your ML models. You can easily extract the results of the [scan feature](../../scan/index.rst) using the following code:

```
from giskard import Dataset, Model

my_dataset = Dataset(...)
my_model = Model(...)

scan_result = giskard.scan(my_model, my_dataset)
test_suite = scan_result.generate_test_suite("My first test suite")
test_suite.run()[1]
```