from ...datasets.base import Dataset
from ...ml_worker.testing.registry.decorators import test
from ...ml_worker.testing.test_result import TestResult
from ...models.base import BaseModel


@test(name="LLM response validation using GPT-4", tags=["llm", "GPT-4"])
def test_llm_response_validation(
    model: BaseModel,
    dataset: Dataset,
    test_case: str,
    threshold: float = 0.5,
):
    """Tests that the rate of generated answer is over a threshold for a given test case.

    Overconfident predictions are defined as predictions where the model
    assigned a large probability to the wrong label. We quantify this as the
    difference between the largest probability assigned to a label and the
    probability assigned to the correct label (this will be 0 if the model
    made the correct prediction). If this is larger than a threshold
    (`p_threshold`, typically determined automatically depending on the number
    of classes), then the prediction is considered overconfident.
    We then calculate the rate of overconfident predictions as the number of
    overconfident samples divided by the total number of wrongly predicted
    samples, and check that it is below a user-specified threshold.

    Arguments:
        model(BaseModel): The generative model to test.
        dataset(Dataset): The dataset to test the model on.
        test_case(str): The test case used to evaluate the response generated by the model
            must be explicit and clear in order to be interpreted properly
            Good assertions
              - The model response must be a JSON valid that respect the following schema
              - The model response must be an apologies with an explanation of the model scope if the question is out of scope (not related to the Pandas Python library)
            Bad assertion
              - A valid json answer
              - Answer to pandas documentation
        threshold(float, optional): The threshold for good response rate, i.e. the min ratio of responses that pass the assertion. Default is 0.50 (50%).
    """
    from ...llm.utils.validate_test_case import validate_test_case

    predictions = model.predict(dataset).prediction

    passed = validate_test_case(model, test_case, dataset.df, predictions)
    metric = len([result for result in passed if result]) / len(predictions)

    return TestResult(
        actual_slices_size=[len(dataset)],
        metric=metric,
        passed=bool(metric > threshold),
    )
